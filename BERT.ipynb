{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw3JCclyAlDI"
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T13:07:54.995889Z",
     "iopub.status.busy": "2024-06-09T13:07:54.995422Z",
     "iopub.status.idle": "2024-06-09T13:07:56.657492Z",
     "shell.execute_reply": "2024-06-09T13:07:56.655825Z",
     "shell.execute_reply.started": "2024-06-09T13:07:54.995854Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYWHU49hAoAe"
   },
   "outputs": [],
   "source": [
    "import warnings \n",
    "\n",
    "import os\n",
    "import wget\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiUVj6Hb42nA"
   },
   "source": [
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T13:08:23.242087Z",
     "iopub.status.busy": "2024-06-09T13:08:23.241414Z",
     "iopub.status.idle": "2024-06-09T13:08:25.477822Z",
     "shell.execute_reply": "2024-06-09T13:08:25.476312Z",
     "shell.execute_reply.started": "2024-06-09T13:08:23.242046Z"
    },
    "id": "OYsXU7Ix5BlN",
    "outputId": "bbb4a3f0-8616-49e6-81a1-644e27ec75c5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Importing the datasets\n",
    "\n",
    "df_train = pd.read_csv(\"/kaggle/input/kuc-hackathon-winter-2018/drugsComTrain_raw.csv\")\n",
    "print (\"The shape of the train set given is : \", df_train.shape)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T13:08:39.410433Z",
     "iopub.status.busy": "2024-06-09T13:08:39.410028Z",
     "iopub.status.idle": "2024-06-09T13:08:39.470629Z",
     "shell.execute_reply": "2024-06-09T13:08:39.468978Z",
     "shell.execute_reply.started": "2024-06-09T13:08:39.410381Z"
    },
    "id": "5mPvmdcu5i2r",
    "outputId": "309026ec-8f22-495e-919c-520eaf304093",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train.dropna(subset=['condition'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T13:09:28.082252Z",
     "iopub.status.busy": "2024-06-09T13:09:28.081726Z",
     "iopub.status.idle": "2024-06-09T13:09:28.107172Z",
     "shell.execute_reply": "2024-06-09T13:09:28.105528Z",
     "shell.execute_reply.started": "2024-06-09T13:09:28.082212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['uniqueID', 'date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T13:09:50.802452Z",
     "iopub.status.busy": "2024-06-09T13:09:50.801870Z",
     "iopub.status.idle": "2024-06-09T13:09:52.316309Z",
     "shell.execute_reply": "2024-06-09T13:09:52.314795Z",
     "shell.execute_reply.started": "2024-06-09T13:09:50.802412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T13:10:04.367001Z",
     "iopub.status.busy": "2024-06-09T13:10:04.366570Z",
     "iopub.status.idle": "2024-06-09T13:10:04.376628Z",
     "shell.execute_reply": "2024-06-09T13:10:04.374839Z",
     "shell.execute_reply.started": "2024-06-09T13:10:04.366968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_review(review):\n",
    "    # Convert to lowercase\n",
    "    review = review.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    review = review.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize the review\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    \n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Join the cleaned tokens back together\n",
    "    cleaned_review = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T13:10:31.471916Z",
     "iopub.status.busy": "2024-06-09T13:10:31.471506Z",
     "iopub.status.idle": "2024-06-09T13:10:33.261254Z",
     "shell.execute_reply": "2024-06-09T13:10:33.259576Z",
     "shell.execute_reply.started": "2024-06-09T13:10:31.471884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-06-09T13:11:10.200206Z",
     "iopub.status.busy": "2024-06-09T13:11:10.199731Z",
     "iopub.status.idle": "2024-06-09T13:11:11.800681Z",
     "shell.execute_reply": "2024-06-09T13:11:11.799022Z",
     "shell.execute_reply.started": "2024-06-09T13:11:10.200169Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train['review'] = df_train['review'].apply(lambda x: clean_review(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8Y6nTeW8Fib",
    "outputId": "8708f39b-baf6-4aa6-fbfe-e8a502dba7a7"
   },
   "outputs": [],
   "source": [
    "print(\"Retrieving Features Dataset\")\n",
    "\n",
    "sentences = df.review.values\n",
    "labels = df['rating']\n",
    "drug_name = df.drugName.values\n",
    "condition = df.condition.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_F3aZPqdePo3"
   },
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAO0E0UDSbQx",
    "outputId": "1fdb2e0a-8a61-4a04-c84a-5ba58271acdb"
   },
   "outputs": [],
   "source": [
    "labels = labels.apply(lambda x: 1 if x >= 5.0 else 0)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "print(\"Encoding Labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsCb1LzP94ik"
   },
   "source": [
    "### Bert Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYWmhDFx8j9Z",
    "outputId": "2fc07c6f-9fdb-4740-9a30-57f057fa2f00"
   },
   "outputs": [],
   "source": [
    "print('Downloading BERT tokenizer...')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0XzvtDS_Sq3"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qk1y0Gc-OoK_",
    "outputId": "74f55d64-0c69-4f2d-cbc2-0afc89fc6027"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "input_ids = []\n",
    "attention_mask = []\n",
    "\n",
    "for s in sentences:\n",
    "\n",
    "    input_encoded = tokenizer.encode_plus(\n",
    "                        s,                      \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = 64,           \n",
    "                        truncation = True,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   \n",
    "                        return_tensors = 'pt',    \n",
    "                   )\n",
    "      \n",
    "    input_ids.append(input_encoded['input_ids'])\n",
    "  \n",
    "    attention_mask.append(input_encoded['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_mask = torch.cat(attention_mask, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "print('Tokenization Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqN2kjflBZ8a"
   },
   "source": [
    "### Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJUosWUgRRr_",
    "outputId": "2ea31627-639c-44c2-c3c6-cfbc6927b766"
   },
   "outputs": [],
   "source": [
    "dfs = TensorDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "size_train = int(0.9 * len(dfs))\n",
    "size_val = len(dfs) - size_train\n",
    "\n",
    "train_data, val_data = random_split(dfs, [size_train, size_val])\n",
    "\n",
    "print(\"{:,} is the training dataset size\".format(size_train))\n",
    "print(\"{:,} is the validation dataset size\".format(size_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5FVOe2TD2s3"
   },
   "source": [
    "### Batch Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ziccrq0fC4hx",
    "outputId": "16eabf6e-1b86-453c-e2b5-b1916e0c4d27"
   },
   "outputs": [],
   "source": [
    "bs = 100\n",
    "\n",
    "train_dl = DataLoader(\n",
    "            train_data,\n",
    "            sampler = RandomSampler(train_data),\n",
    "            batch_size = bs\n",
    ")\n",
    "\n",
    "valid_dl = DataLoader(\n",
    "            val_data,\n",
    "            sampler = SequentialSampler(val_data),\n",
    "            batch_size = bs\n",
    ")\n",
    "\n",
    "print(\"Batch Sampling Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxSDk9-1Eq_H"
   },
   "source": [
    "### BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_zG29mpEeAt",
    "outputId": "87b86e15-70a2-406e-8b17-158746394aee"
   },
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "mod = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")\n",
    "\n",
    "mod.to(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ljkwzYsJ_KK"
   },
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D87kPyq1HsjD",
    "outputId": "c77867d3-aa1f-4cdb-a15e-833dd303c1e2"
   },
   "outputs": [],
   "source": [
    "opt = AdamW(\n",
    "    mod.parameters(),\n",
    "    lr = 3e-5,\n",
    "    eps = 1e-8\n",
    ")\n",
    "\n",
    "print(\"Optimizer Initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcIoyUsaKeJX"
   },
   "source": [
    "### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BEocJciKWL7",
    "outputId": "31dcea3a-97ee-4eca-cb97-b34e229406dc"
   },
   "outputs": [],
   "source": [
    "epoch = 3\n",
    "ts = len(train_dl) * epoch\n",
    "\n",
    "sch = get_linear_schedule_with_warmup(\n",
    "    opt,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = ts\n",
    ")\n",
    "\n",
    "print(\"Scheduler Initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Krbbq9XLiVw"
   },
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LNJB1Z2Led3",
    "outputId": "882256cc-ff7a-46c6-bb57-c0eff98621c1"
   },
   "outputs": [],
   "source": [
    "def acc(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis = -1).flatten()\n",
    "    labels_flat = labels\n",
    "\n",
    "    return np.sum(preds_flat == labels_flat)/len(preds_flat)\n",
    "\n",
    "print(\"Accuracy Function Defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJXQvsFeNXYC"
   },
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_EyfnX1NAxf",
    "outputId": "749bfcac-1017-4ef7-a328-c9692a41c13a"
   },
   "outputs": [],
   "source": [
    "def t(s):\n",
    "    p = int(round(s))\n",
    "    return str(datetime.timedelta(seconds = p))\n",
    "\n",
    "print(\"Time Function Defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmlMxL40WZE1"
   },
   "source": [
    "### Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kWZoqQCsYpO"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "stats = []\n",
    "\n",
    "tot_t0 = time.time()\n",
    "\n",
    "for i in range(0, epoch):\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Training epoch {} / {}\".format(i + 1, epoch))\n",
    "    print(\"Training.......................\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    tot_train_loss = 0\n",
    "\n",
    "    mod.train()\n",
    "\n",
    "    for s, b in enumerate(train_dl):\n",
    "\n",
    "        if s % bs == 0 and not s == 0:\n",
    "\n",
    "            timef = t(time.time() - t0)\n",
    "\n",
    "            print(\"Batch {} of {} has elasped in {}\".format(s, len(train_dl), timef))\n",
    "\n",
    "        input_ids_d = b[0].to(dev)\n",
    "        attention_mask_d = b[1].to(dev)\n",
    "        labels_d = b[2].to(dev)\n",
    "\n",
    "        mod.zero_grad()\n",
    "\n",
    "        cost, logits = mod(\n",
    "              input_ids = input_ids_d,\n",
    "              attention_mask = attention_mask_d,\n",
    "              labels = labels_d,\n",
    "              token_type_ids = None,\n",
    "              return_dict = False\n",
    "        )\n",
    "\n",
    "        tot_train_loss += cost.item()\n",
    "\n",
    "        cost.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(mod.parameters(), 1.0)\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        sch.step()\n",
    "\n",
    "    avg_tloss = tot_train_loss / len(train_dl)\n",
    "\n",
    "    train_time = t(time.time() - t0)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Average Training Loss is {}\".format(avg_tloss))\n",
    "    print(\"Training Time per epoch is {}\".format(train_time))\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Validating.......................\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    mod.eval()\n",
    "\n",
    "    tot_eval_loss = 0\n",
    "    tot_eval_steps = 0\n",
    "\n",
    "    for s, b in enumerate(valid_dl):\n",
    "\n",
    "        input_ids_d = b[0].to(dev)\n",
    "        attention_mask_d = b[1].to(dev)\n",
    "        labels_d = b[2].to(dev)\n",
    "\n",
    "        mod.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            cost, logits = mod(\n",
    "                               input_ids = input_ids_d,\n",
    "                               attention_mask = attention_mask_d,\n",
    "                               labels = labels_d,\n",
    "                               token_type_ids = None,\n",
    "                               return_dict = False\n",
    "                               )\n",
    "\n",
    "        tot_eval_loss += cost.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        labelx = labels_d.detach().cpu().numpy()\n",
    "\n",
    "    avg_vloss = tot_eval_loss / len(valid_dl)\n",
    "    valid_time = t(time.time() - t0)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Average Validation Loss is {}\".format(avg_vloss))\n",
    "    print(\"Validation Time per epoch is {}\".format(valid_time))\n",
    "\n",
    "    stats.append({\n",
    "        'epoch' : i + 1,\n",
    "        'Training Loss' : avg_tloss,\n",
    "        'Training Time' : train_time,\n",
    "        'Validation Loss' : avg_vloss,\n",
    "        'Validation Time' : valid_time\n",
    "    })\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Completed!!!\")\n",
    "print(\"Total Time Taken for Training and Validation is {:}\".format(t(time.time() - tot_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dtpp0iBUG-Tb"
   },
   "source": [
    "### Save Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DkBMHnFLI54",
    "outputId": "5dd01338-bfa9-4ec5-ac6d-240c1a5a5c79"
   },
   "outputs": [],
   "source": [
    "torch.save(mod.state_dict(), \"/content/drive/MyDrive/BERT Models/BERT_Weights.pt\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLjAZ7J-Zukd"
   },
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFecsb-6MQzT"
   },
   "outputs": [],
   "source": [
    "statistics = pd.DataFrame(data = stats)\n",
    "statistics = statistics.set_index('epoch')\n",
    "\n",
    "statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjTsBAI3crHy"
   },
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvVVEXOG0r1C",
    "outputId": "5d6bc4cd-f940-4983-9d5b-3552b150b7c6"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"./drugsComTest_raw.tsv\", delimiter = \"\\t\", header = 0, names = [None, \"drugName\", \"condition\", \"review\", \"rating\", \"date\", \"usefulCount\"])\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shDEVBRKbPr2",
    "outputId": "483d2d09-c838-4ffa-8b82-3f693f42c380"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Retrieving Features Dataset\\n\")\n",
    "sentences = df_test.review.values\n",
    "labels = df_test['rating']\n",
    "drug_name = df_test.drugName.values\n",
    "condition = df_test.condition.values\n",
    "\n",
    "labels = labels.apply(lambda x: 1 if x >= 5.0 else 0)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "print(\"Encoding Labels\\n\")\n",
    "\n",
    "input_ids = []\n",
    "attention_mask = []\n",
    "\n",
    "for s in sentences:\n",
    "  \n",
    "    input_encoded = tokenizer.encode_plus(\n",
    "                        s,                     \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = 64, \n",
    "                        truncation = True,        \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,  \n",
    "                        return_tensors = 'pt',     \n",
    "                   )\n",
    "       \n",
    "    input_ids.append(input_encoded['input_ids'])\n",
    "    attention_mask.append(input_encoded['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim = 0)\n",
    "attention_mask = torch.cat(attention_mask, dim = 0)\n",
    "labels = torch.tensor(labels)\n",
    "  \n",
    "bs = 100\n",
    "\n",
    "pred_data = TensorDataset(input_ids, attention_mask, labels)\n",
    "pred_sampler = SequentialSampler(pred_data)\n",
    "pred_dl = DataLoader(pred_data, sampler = pred_sampler, batch_size = bs)\n",
    "\n",
    "print('Test Data prepared')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SB_vZQ7EfmCl"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MgCcuaHodk_q",
    "outputId": "2fc91da3-75e9-478d-9c4d-ebc14b1dd334"
   },
   "outputs": [],
   "source": [
    "test_acc = 0\n",
    "\n",
    "mod.eval()\n",
    "\n",
    "preds, tl = [], []\n",
    "for s, b in enumerate(pred_dl):\n",
    "\n",
    "    b = tuple(t.to(dev) for t in b)\n",
    "\n",
    "    input_idsx, attention_maskx, labelsx = b\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outs = mod(\n",
    "            input_ids = input_idsx, \n",
    "            attention_mask = attention_maskx, \n",
    "            token_type_ids = None, \n",
    "            )\n",
    "    \n",
    "        logits = outs[0]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        labels = labelsx.to('cpu').numpy()\n",
    "\n",
    "        ta = acc(logits, labels)\n",
    "        test_acc =  test_acc + ta\n",
    "\n",
    "        preds.append(logits)\n",
    "        tl.append(labels)\n",
    "\n",
    "test_acc = (test_acc / len(pred_dl)) * 100\n",
    "\n",
    "print('Prediction Done!')\n",
    "print('Test Accuracy is {}%'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPRPQ_odMe6h"
   },
   "source": [
    "### Performance Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMO7RphziG-a"
   },
   "source": [
    "### Matthew's Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IogIm7yShjta",
    "outputId": "1aac92fe-9f16-4a9f-ae21-209a5d664dfd"
   },
   "outputs": [],
   "source": [
    "mat = []\n",
    "\n",
    "for i in range(len(tl)):\n",
    "\n",
    "    pred_lab = np.argmax(preds[i], axis = 1).flatten()\n",
    "\n",
    "    mat_set = matthews_corrcoef(tl[i], pred_lab)\n",
    "    mat.append(mat_set)\n",
    "\n",
    "print(\"Matthews's Correlation Coefficients Obtained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ts2p1mIti7Qi"
   },
   "source": [
    "### MCC Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9oTJ7cMFi5ps",
    "outputId": "1267251c-617c-46a8-e3bb-a5afbdab3705"
   },
   "outputs": [],
   "source": [
    "mat_50  = mat[:50] \n",
    "\n",
    "fig = sns.barplot(x = list(range(len(mat_50))), y = mat_50)\n",
    "\n",
    "plt.title('MCC Graph')\n",
    "plt.xlabel('#Batch')\n",
    "plt.ylabel('MCC Score')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62ZhnNVnkYqN"
   },
   "source": [
    "### Final MCC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrHdwwNkkKCo",
    "outputId": "dcff1057-a6e2-4b12-b42d-d6839b7d8f8d"
   },
   "outputs": [],
   "source": [
    "predx = np.concatenate(preds, axis = 0)\n",
    "predm = np.argmax(predx, axis = 1).flatten()\n",
    "\n",
    "labelm = np.concatenate(tl, axis = 0)\n",
    "\n",
    "final_mcc = matthews_corrcoef(labelm, predm)\n",
    "print('The Final MCC Score is {}'.format(final_mcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6tEzEOWMKvT"
   },
   "source": [
    "### Line Graph for Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hawEAUkIMUJs",
    "outputId": "9309fcc0-fdd3-414e-907a-5fa0bbd94fc2"
   },
   "outputs": [],
   "source": [
    "sns.set(style = 'darkgrid')\n",
    "sns.set(font_scale = 1.5)\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "plt.plot(statistics['Training Loss'], 'b-o', label = 'Training Loss')\n",
    "plt.plot(statistics['Validation Loss'], 'g-o', label = 'Validation Loss')\n",
    "\n",
    "plt.title('Performance Visualization - Loss')\n",
    "plt.xlabel('#Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xticks(statistics.index, statistics.index + 1)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_XopoovKgTN"
   },
   "source": [
    "### Bar Plot for Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zh7_IooY_PE3",
    "outputId": "96955af9-5a25-4799-f2a6-150866dd337a"
   },
   "outputs": [],
   "source": [
    "bar_x = ['Validation Accuracy', 'Test Accuracy']\n",
    "bar_y = [val_acc, test_acc]\n",
    "db = {'Accuracy' : bar_x, 'Percentage' : bar_y}\n",
    "db = pd.DataFrame(db)\n",
    "sns.barplot(x = 'Accuracy', y = 'Percentage', hue = 'Accuracy', data = db, palette = \"husl\", dodge = False)\n",
    "plt.title('Performance Visualization - Accuracy')\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), shadow=True, ncol=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIk1laaEKFVD"
   },
   "source": [
    "### Multiple Pie Charts for Conditions Distributions per Drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6QJyV46XFb5",
    "outputId": "003090bc-41bd-4746-c3ab-fc2021279a74"
   },
   "outputs": [],
   "source": [
    "print(\"Drug <-- Conditions\")\n",
    "\n",
    "for d in drugs:\n",
    "    df1 = df_test.loc[df_test[\"drugName\"] == d]\n",
    "    df1 = df1[\"condition\"].value_counts().rename_axis(\"condition\").reset_index(name=\"count\")\n",
    "\n",
    "    plt.pie(x = df1[\"count\"], labels = df1[\"condition\"], autopct = \"%1.1f%%\")\n",
    "    plt.title(d)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo6S5p6KKWHE"
   },
   "source": [
    "### Multiple Heatmaps for Drug Recommendation based on Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16-WQZ7-o0XJ",
    "outputId": "95761fc4-9115-4144-9a6d-50976b4c8c95"
   },
   "outputs": [],
   "source": [
    "print(\"Drug <-- Conditions <-- Reccomend (Yes / No)\")\n",
    "\n",
    "for d in drugs:\n",
    "    df2 = df_test.loc[df_test[\"drugName\"] == d]\n",
    "    conditions = df2[\"condition\"].unique()\n",
    "\n",
    "    Yes = []\n",
    "    No = []\n",
    "\n",
    "    for c in conditions:\n",
    "        df3 = df2.loc[df2[\"condition\"] == c]\n",
    "        dict3 = df3[\"rating\"].value_counts().to_dict()\n",
    "        y = dict3.get(1, 0)\n",
    "        n = dict3.get(0, 0)\n",
    "        Yes.append(y)\n",
    "        No.append(n)\n",
    "\n",
    "    classes = ['Reccomend', 'Do Not Reccomend']\n",
    "    vals = np.column_stack((Yes, No))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(vals, cmap=\"Dark2\")\n",
    "    ax.grid(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(len(classes)))\n",
    "    ax.set_yticks(np.arange(len(conditions)))\n",
    "\n",
    "    ax.set_xticklabels(classes)\n",
    "    ax.set_yticklabels(conditions)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    for i in range(len(conditions)):\n",
    "        for j in range(len(classes)):\n",
    "            text = ax.text(j, i, \n",
    "                           vals[i, j],ha=\"center\", \n",
    "                           va=\"center\",color=\"w\", \n",
    "                           fontweight = \"bold\", fontsize = 15)\n",
    "\n",
    "    ax.set_title(d)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Drug Recommendation System based on the Condition of the Patient using Bidirectional Encoder Representations from Transformers.ipynb",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 76158,
     "sourceId": 171475,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
